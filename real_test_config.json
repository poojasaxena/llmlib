{
    "model_config": {
      "d_model": 64,
      "n_heads": 4,
      "n_layers": 2,
      "max_position_embeddings": 128,
      "dropout": 0.1,
      "num_embeddings": 1000
    },
    "training_config": {
      "batch_size": 2,
      "block_size": 128,
      "learning_rate": 1e-3,
      "train_steps": 50,
      "num_epochs": 1,
      "eval_interval": 25,
      "eval_iters": 10,
      "warmup_steps": 5,
      "weight_decay": 1e-4
    },
    "project_metadata": {
      "model_name": "tiny-test-gpt",
      "model_save_path": "test_output/models/",
      "tokenizer_save_path": "test_output/tokenizer.json",
      "data_path": "test_data",
      "data_file": "train.txt", 
      "val_file": "val.txt",
      "test_file": "test.txt",
      "max_seq_length": 128,
      "max_new_tokens": 50
    },
    "tokenizer_config": {
      "type": "byte_bpe",
      "vocab_size": 1000,
      "min_freq": 1,
      "special_tokens": ["<pad>", "<unk>", "<bos>", "<eos>"]
    }
}
